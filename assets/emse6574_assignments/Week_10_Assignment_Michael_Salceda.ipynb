{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_10_Assignment_Michael_Salceda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhHGR81S/bcDv4nwOBmnHJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp8WLe0YTO95"
      },
      "source": [
        "# Week 10 Assignment\n",
        "Find a source of text and create a bag-of-words representation. Build a simple sentiment analyzer from scratch without using any sentiment packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x15inQ_4lEsa"
      },
      "source": [
        "## Data Loading\n",
        "The data I'm using is the Newsgroups data from scikit-learn. While it is mainly used for classification of newsgroups, we can still do something in regards to sentiment analysis with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pgPUILYawCY",
        "outputId": "0f463068-3091-477a-fece-a44c22e56a47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        " import pandas as pd\n",
        " from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_data = pd.DataFrame(\n",
        "    fetch_20newsgroups(\n",
        "        subset = 'train',\n",
        "        categories = ['comp.graphics'], \n",
        "        shuffle = True, \n",
        "        random_state = 1\n",
        "    ).data,\n",
        "    columns = ['text']\n",
        ")\n",
        "newsgroups_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: davidr@rincon.ema.rockwell.com (David J....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: seth@north6.acpub.duke.edu (Seth Wanders...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: camter28@astro.ocis.temple.edu (Carter A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: crussell@netcom.com (Chris Russell)\\nSub...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: brentb@tamsun.tamu.edu (Brent)\\nSubject:...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  From: davidr@rincon.ema.rockwell.com (David J....\n",
              "1  From: seth@north6.acpub.duke.edu (Seth Wanders...\n",
              "2  From: camter28@astro.ocis.temple.edu (Carter A...\n",
              "3  From: crussell@netcom.com (Chris Russell)\\nSub...\n",
              "4  From: brentb@tamsun.tamu.edu (Brent)\\nSubject:..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTQ2Vc9uWwuD"
      },
      "source": [
        "## Data Cleaning & Preprocessing\n",
        "We are interested in the `text` column so we should clean up that column first using some text preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5A5Y2jtYNLj",
        "outputId": "ebcea78d-2eb0-42ec-c11a-5b7a05ab17d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# Chaining some preprocessing steps together, mainly:\n",
        "# 1) Lowercasing text field\n",
        "# 2) Removing punctuation\n",
        "# 3) Replacing newline characters with spaces\n",
        "# 4) Removing numbers\n",
        "newsgroups_data['text_cleaned'] = newsgroups_data['text'] \\\n",
        "    .str.lower() \\\n",
        "    .str.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))) \\\n",
        "    .str.replace('\\n', ' ') \\\n",
        "    .str.replace('\\d+', '')\n",
        "    \n",
        "# Remove stopwords from the cleaned up text field\n",
        "stop_words = stopwords.words('english')\n",
        "newsgroups_data['text_cleaned'] = newsgroups_data['text_cleaned'].apply(\n",
        "    lambda row: ' '.join([word for word in row.split() if word not in stop_words])\n",
        ")\n",
        "\n",
        "# Lemmatize words in the text field. This lemmatizing\n",
        "# step isn't perfect because I am not determining the\n",
        "# POS (part-of-speech) tag so by default, the lemmatizer\n",
        "# assumes each word is a noun and tries to find the lemma\n",
        "# for that form of the word.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "newsgroups_data['text_cleaned'] = newsgroups_data['text_cleaned'].apply(\n",
        "    lambda row: ' '.join([lemmatizer.lemmatize(word) for word in row.split()])\n",
        ")\n",
        "\n",
        "print(f'=====TEXT BEFORE PROCESSING===== \\n\"{newsgroups_data[\"text\"][0]}\"')\n",
        "print(f'=====TEXT AFTER PROCESSING===== \\n\"{newsgroups_data[\"text_cleaned\"][0]}\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "=====TEXT BEFORE PROCESSING===== \n",
            "\"From: davidr@rincon.ema.rockwell.com (David J. Ray)\n",
            "Subject: Re: Fractals? what good are they?\n",
            "Organization: Rockwell International\n",
            "X-Newsreader: Tin 1.1 PL5\n",
            "Lines: 16\n",
            "\n",
            "In regards to fractal commpression, I have seen 2 fractal compressed \"movies\".\n",
            "They were both fairly impressive.  The first one was a 64 gray scale \"movie\" of\n",
            "Casablanca, it was 1.3MB and had 11 minutes of 13 fps video.  It was a little\n",
            "grainy but not bad at all.  The second one I saw was only 3 minutes but it\n",
            "had 8 bit color with 10fps and measured in at 1.2MB.\n",
            "\n",
            "I consider the fractal movies a practical thing to explore.  But unlike many \n",
            "other formats out there, you do end up losing resolution.  I don't know what\n",
            "kind of software/hardware was used for creating the \"movies\" I saw but the guy\n",
            "that showed them to me said it took 5-15 minutes per frame to generate.  But as\n",
            "I said above playback was 10 or more frames per second.  And how else could you\n",
            "put 11 minutes on one floppy disk?\n",
            "\n",
            "davidr@rincon.ema.rockwell.com\n",
            "My opinions are my own except where they are shared by others in which case I \n",
            "will probably change my mind.\n",
            "\"\n",
            "=====TEXT AFTER PROCESSING===== \n",
            "\"davidr rincon ema rockwell com david j ray subject fractal good organization rockwell international x newsreader tin pl line regard fractal commpression seen fractal compressed movie fairly impressive first one gray scale movie casablanca mb minute fps video little grainy bad second one saw minute bit color fps measured mb consider fractal movie practical thing explore unlike many format end losing resolution know kind software hardware used creating movie saw guy showed said took minute per frame generate said playback frame per second else could put minute one floppy disk davidr rincon ema rockwell com opinion except shared others case probably change mind\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-28ne0_5hFA4"
      },
      "source": [
        "## Bag-of-Words\n",
        "scikit-learn has a built-in method of creating a bag-of-words representation of a series of text using the `CountVectorizer` method. I will use that in the following cells. The thing to note is that `CountVectorizer` creates a sparse vector representation so I also convert this sparse representation to a dense one in order to view it. In practice, it should be kept as a sparse vector for performance reasons since a dense vector will take up a bunch of memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfMzQaB-iaav",
        "outputId": "e190eea0-8976-464e-98bd-cd1815b3adca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "text_bow = vectorizer.fit_transform(newsgroups_data['text_cleaned'])\n",
        "text_bow_dense = pd.DataFrame(text_bow.todense(), columns = vectorizer.get_feature_names())\n",
        "text_bow_dense.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aaaa</th>\n",
              "      <th>aad</th>\n",
              "      <th>aalborg</th>\n",
              "      <th>aamrl</th>\n",
              "      <th>aangeboden</th>\n",
              "      <th>aantal</th>\n",
              "      <th>aao</th>\n",
              "      <th>aaoepp</th>\n",
              "      <th>aaplay</th>\n",
              "      <th>aarhus</th>\n",
              "      <th>aarnet</th>\n",
              "      <th>aau</th>\n",
              "      <th>ab</th>\n",
              "      <th>abad</th>\n",
              "      <th>abandon</th>\n",
              "      <th>abbreviation</th>\n",
              "      <th>abc</th>\n",
              "      <th>abekas</th>\n",
              "      <th>abel</th>\n",
              "      <th>aberdeen</th>\n",
              "      <th>abild</th>\n",
              "      <th>abildskov</th>\n",
              "      <th>ability</th>\n",
              "      <th>ablaze</th>\n",
              "      <th>able</th>\n",
              "      <th>abo</th>\n",
              "      <th>abort</th>\n",
              "      <th>abp</th>\n",
              "      <th>abpsoft</th>\n",
              "      <th>abrash</th>\n",
              "      <th>abraxis</th>\n",
              "      <th>absence</th>\n",
              "      <th>absent</th>\n",
              "      <th>absolute</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>abstact</th>\n",
              "      <th>abstract</th>\n",
              "      <th>abstractsoft</th>\n",
              "      <th>abuse</th>\n",
              "      <th>...</th>\n",
              "      <th>zbuffering</th>\n",
              "      <th>zbww</th>\n",
              "      <th>zc</th>\n",
              "      <th>zcat</th>\n",
              "      <th>zealand</th>\n",
              "      <th>zeit</th>\n",
              "      <th>zemcik</th>\n",
              "      <th>zen</th>\n",
              "      <th>zenith</th>\n",
              "      <th>zenkar</th>\n",
              "      <th>zeno</th>\n",
              "      <th>zentrum</th>\n",
              "      <th>zephyr</th>\n",
              "      <th>zero</th>\n",
              "      <th>zeus</th>\n",
              "      <th>zhao</th>\n",
              "      <th>zhenghao</th>\n",
              "      <th>ziedman</th>\n",
              "      <th>zillion</th>\n",
              "      <th>zip</th>\n",
              "      <th>zipped</th>\n",
              "      <th>zippy</th>\n",
              "      <th>zirkel</th>\n",
              "      <th>zola</th>\n",
              "      <th>zoo</th>\n",
              "      <th>zool</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zooming</th>\n",
              "      <th>zopfi</th>\n",
              "      <th>zorg</th>\n",
              "      <th>zorn</th>\n",
              "      <th>zrz</th>\n",
              "      <th>zsoft</th>\n",
              "      <th>zt</th>\n",
              "      <th>zug</th>\n",
              "      <th>zurich</th>\n",
              "      <th>zvi</th>\n",
              "      <th>zyeh</th>\n",
              "      <th>zyxel</th>\n",
              "      <th>ªl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>583</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 10741 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     aa  aaaa  aad  aalborg  aamrl  ...  zurich  zvi  zyeh  zyxel  ªl\n",
              "583   0     0    0        0      0  ...       0    0     0      0   0\n",
              "212   0     0    0        0      0  ...       0    0     0      0   0\n",
              "43    0     0    0        0      0  ...       0    0     0      0   0\n",
              "309   0     0    0        0      0  ...       0    0     0      0   0\n",
              "235   0     0    0        0      0  ...       0    0     0      0   0\n",
              "\n",
              "[5 rows x 10741 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVoYeKr5pJUB"
      },
      "source": [
        "## Sentiment Analyzer\n",
        "A brute-force way to build a simple sentiment analyzer would be to simply count the number of occurrences of \"positive\" words and \"negative\" words. The sentiment score of the analyzer would simply be the difference between the positive count and negative count. If the final score is positive, that means there are more positive words found so the overall sentiment would be positive. If the final score was negative, there are more negative words so overall sentiment would be negative. Finally, if the final score was 0, there is a tie or no presence of either positive or negative words so it would be neutral."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tF_g5TQlfDo",
        "outputId": "a851dfa3-697e-4fae-f675-68a696b9f80a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def very_simple_sentiment_analyzer(positive_words, negative_words, data):\n",
        "    '''Analyze a bag-of-words representation of texts for sentiment.\n",
        "\n",
        "    This function simply uses counts of positive and negative words\n",
        "    to determine if a particular text field is positive, negative, \n",
        "    of neutral.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    positive_words : list[str]\n",
        "        List of words that are considered positive.\n",
        "\n",
        "    negative_words : list[str]\n",
        "        List of words that are considered negative.\n",
        "\n",
        "    data : pandas DataFrame\n",
        "        A DataFrame containing the bag-of-words representation\n",
        "        of a corpus.\n",
        "    '''\n",
        "    # Sum up all the postive word counts if they exist\n",
        "    positive_counts = data[data.columns.intersection(positive_words)].sum(axis = 1)\n",
        "\n",
        "    # Sum up all the negative word counts if they exist\n",
        "    negative_counts = data[data.columns.intersection(negative_words)].sum(axis = 1)\n",
        "\n",
        "    # Determine the final label based on the total score.\n",
        "    total_score = positive_counts - negative_counts\n",
        "    final_label = np.where(\n",
        "        total_score > 0, \n",
        "        'POSITIVE', \n",
        "        np.where(total_score == 0, 'NEUTRAL', 'NEGATIVE') \n",
        "    )\n",
        "\n",
        "    return final_label\n",
        "\n",
        "positive_words = [\n",
        "    'awesome',\n",
        "    'amazing',\n",
        "    'amaze',\n",
        "    'best',\n",
        "    'bountiful',\n",
        "    'beautiful',\n",
        "    'beauty',\n",
        "    'cool',\n",
        "    'calm',\n",
        "    'dashing',\n",
        "    'delicious',\n",
        "    'decadent',\n",
        "    'dope',\n",
        "    'good',\n",
        "    'impress',\n",
        "    'impressed',\n",
        "    'pretty',\n",
        "    'positive',\n",
        "    'positively'\n",
        "]\n",
        "\n",
        "negative_words = [\n",
        "    'awful',\n",
        "    'atrocious',\n",
        "    'bad',\n",
        "    'broke',\n",
        "    'broken',\n",
        "    'boo',\n",
        "    'bore',\n",
        "    'boring',\n",
        "    'crazy',\n",
        "    'craze',\n",
        "    'cantankerous',\n",
        "    'cranky',\n",
        "    'doom',\n",
        "    'darn',\n",
        "    'death',\n",
        "    'drat',\n",
        "    'dislike',\n",
        "    'ew',\n",
        "    'fault',\n",
        "    'faulty',\n",
        "    'hate',\n",
        "    'icky',\n",
        "    'mean',\n",
        "    'negative',\n",
        "    'rot',\n",
        "    'rotten',\n",
        "    'suck',\n",
        "    'stink'\n",
        "]\n",
        "\n",
        "text_bow_dense['sentiment'] = very_simple_sentiment_analyzer(\n",
        "    positive_words,\n",
        "    negative_words,\n",
        "    text_bow_dense\n",
        ")\n",
        "text_bow_dense['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NEUTRAL     388\n",
              "POSITIVE    144\n",
              "NEGATIVE     52\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WXMVdRCvx6J"
      },
      "source": [
        "### Results Analysis\n",
        "Let's take a look at a \"positive\" text, \"negative\" text, and \"neutral\" text to see how it did. Obviously, I don't expect much since the list of positive and negative words is hardly exhaustive, but it's a start. :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC0jpB9xvnz8",
        "outputId": "e2606ec7-b599-42a1-abc2-ea386f6e92f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "newsgroups_data_analyzed = newsgroups_data.join(text_bow_dense[['sentiment']])\n",
        "positive_sample = newsgroups_data_analyzed[newsgroups_data_analyzed['sentiment'] == 'POSITIVE'].sample(1)\n",
        "negative_sample = newsgroups_data_analyzed[newsgroups_data_analyzed['sentiment'] == 'NEGATIVE'].sample(1)\n",
        "neutral_sample = newsgroups_data_analyzed[newsgroups_data_analyzed['sentiment'] == 'NEUTRAL'].sample(1)\n",
        "\n",
        "print(f'=====POSITIVE SAMPLE===== \\n\"{positive_sample[\"text\"].values[0]}\"\\n\\n====================')\n",
        "print(f'=====NEGATIVE SAMPLE===== \\n\"{negative_sample[\"text\"].values[0]}\"\\n\\n===================')\n",
        "print(f'=====NEUTRAL SAMPLE===== \\n\"{neutral_sample[\"text\"].values[0]}\"\\n\\n====================')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=====POSITIVE SAMPLE===== \n",
            "\"From: markl@hunan.rastek.com (Mark Larsen)\n",
            "Subject: Re: Ray tracer for ms-dos?\n",
            "Organization: Rastek Corporation, Huntsville, AL\n",
            "Lines: 32\n",
            "\n",
            "In article <1r1cqiINNje8@srvr1.engin.umich.edu> tdawson@llullaillaco.engin.umich.edu (Chris Herringshaw) writes:\n",
            ">\n",
            ">Sorry for the repeat of this request, but does anyone know of a good\n",
            ">free/shareware program with which I can create ray-traces and save\n",
            ">them as bit-mapped files?  (Of course if there is such a thing =)\n",
            ">\n",
            ">Thanks in advance\n",
            ">\n",
            ">Daemon\n",
            "\n",
            "There are 2 books published by M&T BOOKS that come with C source code on\n",
            "floppies.  They are:\n",
            "\n",
            "Programming In 3 Dimensions, 3-D Graphics, Ray Traycing, and Animation\n",
            "by: Christopher D. Watkins and Larry Sharp.\n",
            "\n",
            "Photorealism and Ray Tracing in C\n",
            "by: Christopher D. Watkins, Stephen B. Coy, and Mark Finlay.\n",
            "\n",
            "I have the first book and it is a great intro to 3-D, Ray Tracing and\n",
            "Animation.  Most of the programs are on the disk compiled and ready to run.\n",
            "\n",
            "I have only glanced at the second book but it also appears to be good.\n",
            "\n",
            "Hope this helps!\n",
            "Mark Larsen\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "markl@hunan.rastek.com\n",
            "\n",
            "\"This R2 unit has a bad motivator!\"\n",
            "   - Luke, Star Wars\n",
            "\"\n",
            "\n",
            "====================\n",
            "=====NEGATIVE SAMPLE===== \n",
            "\"From: ehgasm2@uts.mcc.ac.uk (Simon Marshall)\n",
            "Subject: How do I compensate for photographic viewpoint and distortion?\n",
            "Reply-To: S.Marshall@dcs.hull.ac.uk\n",
            "Organization: Manchester Computing Centre, Manchester, England\n",
            "Lines: 42\n",
            "\n",
            "Hi to all out there.  We have this problem, and I'm not certain I'm solving it\n",
            "in the correct way.  I was wondering if anyone can shed light on this, or point\n",
            "me in the right place to look...\n",
            "\n",
            "We have an X-ray imaging camera and a metallic tube with a cylindrical hole\n",
            "passing through it at a right angle to the tube's axis:\n",
            "\n",
            "                                                          |\n",
            "                                     ||                   [ image\n",
            "                                                          |\n",
            "  X-ray source ]                     ||                   | screen\n",
            "                            metallic || tube              |\n",
            "                                     ||                   |\n",
            "                                                          |\n",
            "\n",
            "We know source--screen centre distance, radius of the tube, radius of the hole.\n",
            "\n",
            "We do some calculations based on the image of the hole on the screen.  However,\n",
            "the calculations are mathematically highly complex, and must assume that the\n",
            "object's hole projects an image (resembling an ellipse if the tube is not\n",
            "parallel to the screen) in the centre of the screen.  However, it is unlikely\n",
            "that the object is placed so conveniently.  \n",
            "\n",
            "Firstly, we must transform the major and minor axis of the ellipse.  I cannot\n",
            "know what the angle between the tube and screen is.  Do I have to assume that\n",
            "they are parallel to do the transformation?  How do I do this transformation?\n",
            "\n",
            "Secondly, there is a distortion of the image due to the screen being planar\n",
            "(the source--screen distance increases as we move away from the centre of the\n",
            "screen).  How can I compensate the ellipse's axis for this image distortion?\n",
            "\n",
            "So, please can anyone give us a few pointers here?  How do we transform the\n",
            "image so it appears as it would if it were in the centre of the screen, and how\n",
            "do I deal with distortion due to the shape of the screen?\n",
            "\n",
            "We'd appreciate any help, either posted or emailed.\n",
            "\n",
            "Thanks in advance, Simon.\n",
            "-- \n",
            "Simon Marshall, Dept. of Computer Science, University of Hull, Hull HU6 7RX, UK\n",
            "  \"Football isn't about life and death.  It's more important than that.\" Bill\n",
            "Email: S.Marshall@cs.hull.ac.uk   Phone: +44 482 465951  Fax: 466666   Shankley\n",
            "\"\n",
            "\n",
            "===================\n",
            "=====NEUTRAL SAMPLE===== \n",
            "\"From: N020BA@tamvm1.tamu.edu\n",
            "Subject: Help! Need 3-D graphics code/package for DOS!!!\n",
            "Organization: Texas A&M University\n",
            "Lines: 7\n",
            "NNTP-Posting-Host: tamvm1.tamu.edu\n",
            "\n",
            "    Help!! I need code/package/whatever to take 3-D data and turn it into\n",
            "a wireframe surface with hidden lines removed. I'm using a DOS machine, and\n",
            "the code can be in ANSI C or C++, ANSI Fortran or Basic. The data I'm using\n",
            "forms a rectangular grid.\n",
            "   Please post your replies to the net so that others may benefit. IMHO, this\n",
            "is a general interest question.\n",
            "   Thank you!!!!!!\n",
            "\"\n",
            "\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}