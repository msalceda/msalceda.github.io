{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week_11_Assignment_Michael_Salceda.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONlSa+ruJMAfU9ljNlFW+l"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jA9SY8Ok-H5Y"},"source":["# Week 11 Assignment\n","Find a set of texts. Preprocess it and use k-means to generate a topic map."]},{"cell_type":"markdown","metadata":{"id":"YYRVXxP3IChB"},"source":["## Data Loading\n","The data I'm using is the Newsgroups data from scikit-learn. It has posts split up into various \"groups\" so I can use them for comparing how well the k-means clustering does in grouping topics together."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"Vf7e0jDA7PqD","executionInfo":{"status":"ok","timestamp":1605924310987,"user_tz":300,"elapsed":1383,"user":{"displayName":"Michael Salceda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6g2OKLk-DSxGzf_Sd04V8W79UW5dPyuoAgxz8=s64","userId":"17266514555816969541"}},"outputId":"98d5cc9c-d4a3-4c60-99cf-8be0b7c57fb8"},"source":["import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups\n","\n","newsgroups_databunch = fetch_20newsgroups(\n","    subset = 'train',\n","    # Selecting distinct groups here so we can see if k-means\n","    # can even separate distinct topics from each other\n","    categories = [\n","        'comp.graphics',\n","        'alt.atheism',\n","        'misc.forsale',\n","        'rec.sport.hockey',\n","        'sci.space',\n","        'talk.politics.guns'\n","    ], \n","    shuffle = True, \n","    random_state = 1\n",")\n","\n","# The `filenames` attribute has the format\n","# \"/root/scikit_learn_data/20news_home/20news-bydate-train/<CATEGORY>/<POST NUMBER>\"\n","# so we can pull it out from the 6th element after a split on the \"/\" character\n","target_names = pd.Series(newsgroups_databunch.filenames) \\\n","    .apply(lambda x: x.split('/')[5])\n","\n","newsgroups_data = pd.DataFrame(newsgroups_databunch.data, columns = ['text'])\n","newsgroups_data['category'] = target_names\n","newsgroups_data.head()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>From: steinly@topaz.ucsc.edu (Steinn Sigurdsso...</td>\n","      <td>sci.space</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>From: huot@cray.com (Tom Huot)\\nSubject: Re: B...</td>\n","      <td>rec.sport.hockey</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From: Robert Angelo Pleshar &lt;rp16+@andrew.cmu....</td>\n","      <td>rec.sport.hockey</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>From: halat@pooh.bears (Jim Halat)\\nSubject: R...</td>\n","      <td>alt.atheism</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From: jkatz@access.digex.com (Jordan Katz)\\nSu...</td>\n","      <td>sci.space</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text          category\n","0  From: steinly@topaz.ucsc.edu (Steinn Sigurdsso...         sci.space\n","1  From: huot@cray.com (Tom Huot)\\nSubject: Re: B...  rec.sport.hockey\n","2  From: Robert Angelo Pleshar <rp16+@andrew.cmu....  rec.sport.hockey\n","3  From: halat@pooh.bears (Jim Halat)\\nSubject: R...       alt.atheism\n","4  From: jkatz@access.digex.com (Jordan Katz)\\nSu...         sci.space"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"ArcEAxNeL8xq"},"source":["## Data Cleaning & Preprocessing\n","We are interested in the `text` column so we should clean up that column first using some text preprocessing steps."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2DoQVU1L9-Z","executionInfo":{"status":"ok","timestamp":1605924316459,"user_tz":300,"elapsed":6848,"user":{"displayName":"Michael Salceda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6g2OKLk-DSxGzf_Sd04V8W79UW5dPyuoAgxz8=s64","userId":"17266514555816969541"}},"outputId":"fbff1cb8-1307-4655-b036-922f7b9470fb"},"source":["import string\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","# Chaining some preprocessing steps together, mainly:\n","# 1) Lowercasing text field\n","# 2) Remove header information (\"From\", \"Subject\", etc.) by\n","#    splitting on double newlines since that how it looks like\n","#    the text is formatted\n","# 3) Removing punctuation\n","# 4) Replacing newline characters with spaces\n","# 5) Removing numbers\n","newsgroups_data['text_cleaned'] = newsgroups_data['text'] \\\n","    .str.lower() \\\n","    .apply(lambda x: ' '.join(x.split('\\n\\n')[1:])) \\\n","    .str.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))) \\\n","    .str.replace('\\n', ' ') \\\n","    .str.replace('\\d+', '')\n","    \n","# Remove stopwords from the cleaned up text field\n","stop_words = stopwords.words('english')\n","newsgroups_data['text_cleaned'] = newsgroups_data['text_cleaned'].apply(\n","    lambda row: ' '.join([word for word in row.split() if word not in stop_words])\n",")\n","\n","# Lemmatize words in the text field. This lemmatizing\n","# step isn't perfect because I am not determining the\n","# POS (part-of-speech) tag so by default, the lemmatizer\n","# assumes each word is a noun and tries to find the lemma\n","# for that form of the word.\n","lemmatizer = WordNetLemmatizer()\n","newsgroups_data['text_cleaned'] = newsgroups_data['text_cleaned'].apply(\n","    lambda row: ' '.join([lemmatizer.lemmatize(word) for word in row.split()])\n",")\n","\n","print(f'=====TEXT BEFORE PROCESSING===== \\n\"{newsgroups_data[\"text\"][0]}\"')\n","print(f'=====TEXT AFTER PROCESSING===== \\n\"{newsgroups_data[\"text_cleaned\"][0]}\"')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","=====TEXT BEFORE PROCESSING===== \n","\"From: steinly@topaz.ucsc.edu (Steinn Sigurdsson)\n","Subject: Re: New planet/Kuiper object found?\n","Organization: Lick Observatory/UCO\n","Lines: 23\n","Distribution: sci\n","\t<1r9de3INNjkv@gap.caltech.edu>\n","NNTP-Posting-Host: topaz.ucsc.edu\n","In-reply-to: jafoust@cco.caltech.edu's message of 23 Apr 1993 18:44:19 GMT\n","\n","In article <1r9de3INNjkv@gap.caltech.edu> jafoust@cco.caltech.edu (Jeff Foust) writes:\n","\n","   In a recent article jdnicoll@prism.ccs.uwo.ca (James Davis Nicoll) writes:\n","   >\tIf the  new  Kuiper belt object *is*  called 'Karla', the next\n","   >one  should be called 'Smiley'.\n","\n","   Unless I'm imaging things, (always a possibility =) 1992 QB1, the Kuiper Belt\n","   object discovered last year, is known as Smiley.\n","\n","As it happens the _second_ one is Karla. The first one was\n","Smiley. All subject to the vagaries of the IAU of course,\n","but I think they might let this one slide...\n","\n","*  Steinn Sigurdsson   \t\t\tLick Observatory      \t          *\n","*  steinly@lick.ucsc.edu\t\t\"standard disclaimer\"  \t          *\n","*  \"The worst thing you can say to a true revolutionary is that his \t  *\n","*  revolution is unnecessary, that the problems can be corrected without  *\n","*  radical change. Telling people that paradise can be attained without   *\n","*  revolution is treason of the vilest kind.\"  -- H.S. 1993\t\t  * \n","\n","\n","Just had to try out my new .sig# on this forum ;-)\n","\n","\"\n","=====TEXT AFTER PROCESSING===== \n","\"article rdeinnjkv gap caltech edu jafoust cco caltech edu jeff foust writes recent article jdnicoll prism cc uwo ca james davis nicoll writes new kuiper belt object called karla next one called smiley unless imaging thing always possibility qb kuiper belt object discovered last year known smiley happens second one karla first one smiley subject vagary iau course think might let one slide steinn sigurdsson lick observatory steinly lick ucsc edu standard disclaimer worst thing say true revolutionary revolution unnecessary problem corrected without radical change telling people paradise attained without revolution treason vilest kind h try new sig forum\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oK45EBE3UPRe"},"source":["## Bag-of-Words\n","scikit-learn has a built-in method of creating a bag-of-words representation of a series of text using the `CountVectorizer` method. I will use that in the following cells."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"HxU8eHXNUnDa","executionInfo":{"status":"ok","timestamp":1605924316771,"user_tz":300,"elapsed":7155,"user":{"displayName":"Michael Salceda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6g2OKLk-DSxGzf_Sd04V8W79UW5dPyuoAgxz8=s64","userId":"17266514555816969541"}},"outputId":"442d7969-4216-44c6-98cb-abc95dcf987a"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(min_df = 2)\n","text_bow = vectorizer.fit_transform(newsgroups_data['text_cleaned'])\n","text_bow_dense = pd.DataFrame(text_bow.todense(), columns = vectorizer.get_feature_names())\n","text_bow_dense.sample(5)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>aa</th>\n","      <th>aaa</th>\n","      <th>aaah</th>\n","      <th>aadce</th>\n","      <th>aamrl</th>\n","      <th>aantal</th>\n","      <th>aao</th>\n","      <th>aargh</th>\n","      <th>aario</th>\n","      <th>aaron</th>\n","      <th>ab</th>\n","      <th>abad</th>\n","      <th>abandon</th>\n","      <th>abandoned</th>\n","      <th>abbreviation</th>\n","      <th>abc</th>\n","      <th>abel</th>\n","      <th>abetter</th>\n","      <th>abide</th>\n","      <th>abiding</th>\n","      <th>ability</th>\n","      <th>able</th>\n","      <th>abo</th>\n","      <th>aboard</th>\n","      <th>abode</th>\n","      <th>abolished</th>\n","      <th>abomination</th>\n","      <th>abort</th>\n","      <th>aborted</th>\n","      <th>abortion</th>\n","      <th>abotu</th>\n","      <th>aboyko</th>\n","      <th>abraham</th>\n","      <th>abraxis</th>\n","      <th>abridge</th>\n","      <th>abrupt</th>\n","      <th>abruptly</th>\n","      <th>absence</th>\n","      <th>absent</th>\n","      <th>absolut</th>\n","      <th>...</th>\n","      <th>zeta</th>\n","      <th>zettler</th>\n","      <th>zeus</th>\n","      <th>zezel</th>\n","      <th>zhamnov</th>\n","      <th>zhao</th>\n","      <th>zhenghao</th>\n","      <th>zhitnik</th>\n","      <th>zhivov</th>\n","      <th>zi</th>\n","      <th>zilch</th>\n","      <th>zillion</th>\n","      <th>zimring</th>\n","      <th>zip</th>\n","      <th>zipper</th>\n","      <th>zipping</th>\n","      <th>zippy</th>\n","      <th>zlumber</th>\n","      <th>zmed</th>\n","      <th>zmolek</th>\n","      <th>zo</th>\n","      <th>zog</th>\n","      <th>zola</th>\n","      <th>zombie</th>\n","      <th>zombo</th>\n","      <th>zone</th>\n","      <th>zoo</th>\n","      <th>zoology</th>\n","      <th>zoom</th>\n","      <th>zoomed</th>\n","      <th>zooming</th>\n","      <th>zt</th>\n","      <th>zu</th>\n","      <th>zubov</th>\n","      <th>zulu</th>\n","      <th>zupancic</th>\n","      <th>zurich</th>\n","      <th>zvbww</th>\n","      <th>zyeh</th>\n","      <th>zyxel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>893</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1000</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1966</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2738</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2867</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 18206 columns</p>\n","</div>"],"text/plain":["      aa  aaa  aaah  aadce  aamrl  ...  zupancic  zurich  zvbww  zyeh  zyxel\n","893    0    0     0      0      0  ...         0       0      0     0      0\n","1000   0    0     0      0      0  ...         0       0      0     0      0\n","1966   0    0     0      0      0  ...         0       0      0     0      0\n","2738   0    0     0      0      0  ...         0       0      0     0      0\n","2867   0    0     0      0      0  ...         0       0      0     0      0\n","\n","[5 rows x 18206 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"tXiL--M2W6MG"},"source":["## k-Means Clustering\n","Let's try clustering the bag-of-words into topics and see how it does."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNqlWsaxW01B","executionInfo":{"status":"ok","timestamp":1605924356680,"user_tz":300,"elapsed":47057,"user":{"displayName":"Michael Salceda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6g2OKLk-DSxGzf_Sd04V8W79UW5dPyuoAgxz8=s64","userId":"17266514555816969541"}},"outputId":"78a5c3b7-00b3-49c7-b0d8-1f82f8486032"},"source":["from sklearn.cluster import KMeans\n","\n","kmeans = KMeans(\n","    n_clusters = 6, # There are six categories so we should try to make six clusters\n","    random_state = 1\n",")\n","kmeans.fit(text_bow)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n","       n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n","       random_state=1, tol=0.0001, verbose=0)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"I6FvfbRqYZHv","executionInfo":{"status":"ok","timestamp":1605924356681,"user_tz":300,"elapsed":47053,"user":{"displayName":"Michael Salceda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6g2OKLk-DSxGzf_Sd04V8W79UW5dPyuoAgxz8=s64","userId":"17266514555816969541"}},"outputId":"5425f736-3f35-4a43-d09d-f0b3079e4100"},"source":["# Get the predicted clusters from the k-Means\n","newsgroups_data['predicted_cluster'] = kmeans.predict(text_bow)\n","newsgroups_data.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>category</th>\n","      <th>text_cleaned</th>\n","      <th>predicted_cluster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>From: steinly@topaz.ucsc.edu (Steinn Sigurdsso...</td>\n","      <td>sci.space</td>\n","      <td>article rdeinnjkv gap caltech edu jafoust cco ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>From: huot@cray.com (Tom Huot)\\nSubject: Re: B...</td>\n","      <td>rec.sport.hockey</td>\n","      <td>oh excuse wasting bandwidth referring original...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From: Robert Angelo Pleshar &lt;rp16+@andrew.cmu....</td>\n","      <td>rec.sport.hockey</td>\n","      <td>deal bill wirtz apparently blackhawks st louis...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>From: halat@pooh.bears (Jim Halat)\\nSubject: R...</td>\n","      <td>alt.atheism</td>\n","      <td>article bu edu jaeger buphy bu edu gregg jaege...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From: jkatz@access.digex.com (Jordan Katz)\\nSu...</td>\n","      <td>sci.space</td>\n","      <td>ssrt rollout speech delivered col simon p word...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  ... predicted_cluster\n","0  From: steinly@topaz.ucsc.edu (Steinn Sigurdsso...  ...                 0\n","1  From: huot@cray.com (Tom Huot)\\nSubject: Re: B...  ...                 0\n","2  From: Robert Angelo Pleshar <rp16+@andrew.cmu....  ...                 0\n","3  From: halat@pooh.bears (Jim Halat)\\nSubject: R...  ...                 0\n","4  From: jkatz@access.digex.com (Jordan Katz)\\nSu...  ...                 0\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"rnBaxc0KdNzb"},"source":["## k-Means Clustering Evaluation\n","If you have the ground-truth for labels, there are three metrics you can use to evaluate how well your clustering did: completeness, homogeneity, and V-measure.\n","* Completeness: A score from 0 to 1 indicating if all points for a particular category are assigned to the same cluster. Higher is better.\n","* Homogeneity: A score from 0 to 1 indicating how well each cluster is in only containing members of a single category. Higher is better.\n","* V-measure: A score from 0 to 1 that is the harmonic mean of completeness and homogeneity. Higher is better."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4HtViUEZtRX","executionInfo":{"status":"ok","timestamp":1605924356682,"user_tz":300,"elapsed":47048,"user":{"displayName":"Michael Salceda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6g2OKLk-DSxGzf_Sd04V8W79UW5dPyuoAgxz8=s64","userId":"17266514555816969541"}},"outputId":"49d6e180-6dd8-4818-f7b2-dd55a7075c43"},"source":["from sklearn.metrics import completeness_score, homogeneity_score, v_measure_score\n","\n","print(f'Completeness Score: {completeness_score(newsgroups_data[\"category\"], newsgroups_data[\"predicted_cluster\"])}')\n","print(f'Homogeneity Score:  {homogeneity_score(newsgroups_data[\"category\"], newsgroups_data[\"predicted_cluster\"])}')\n","print(f'V-measure Score:    {v_measure_score(newsgroups_data[\"category\"], newsgroups_data[\"predicted_cluster\"])}')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Completeness Score: 0.20366589824401074\n","Homogeneity Score:  0.002036081833853821\n","V-measure Score:    0.004031856527906846\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sJPLTjpkiSDs"},"source":["Looks like k-Means didn't do too good a job with the bag-of-words features in finding the correct clusters. :("]}]}